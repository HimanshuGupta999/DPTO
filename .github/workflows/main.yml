name: Distributed Load Test Pipeline

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

jobs:
  run-test:
    runs-on: ubuntu-latest

    env:
      JMETER_VERSION: ${{ secrets.JMETER_VERSION }}
      MASTER_IP: ${{ secrets.MASTER_IP }}
      SLAVE_IPS: ${{ secrets.SLAVE_IPS }}
      JMETER_DIR: ${{ secrets.JMETER_DIR }}
      SSH_USERNAME: ${{ secrets.SSH_USERNAME }}
      SSH_PASSWORD: ${{ secrets.SSH_PASSWORD }}
      TEST_PLAN_PATH: ${{ secrets.TEST_PLAN_PATH }}
      SSH_PORT_MASTER: ${{ secrets.SSH_PORT_MASTER }}
      SSH_PORT_SLAVE1: ${{ secrets.SSH_PORT_SLAVE1 }}
      SSH_PORT_SLAVE2: ${{ secrets.SSH_PORT_SLAVE2 }}
      RESULT_PATH: ${{ secrets.RESULT_PATH }}
      NUMBER_OF_THREADS: ${{ secrets.NUMBER_OF_THREADS }}
      GEMINI_MODEL_ID: ${{ secrets.GEMINI_MODEL_ID }}
      GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
      PYTHONPATH: ${{ secrets.PYTHONPATH }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          ref: main

      - name: Navigate to distributed-performance-testing
        working-directory: ./distributed-performance-testing
        run: echo "Entered project directory"
    
      - name: Create .env file inside distributed-performance-testing
        working-directory: ./distributed-performance-testing
        run: |
          cat <<EOF > .env
          JMETER_VERSION=${JMETER_VERSION}
          MASTER_IP=${MASTER_IP}
          SLAVE_IPS=${SLAVE_IPS}
          JMETER_DIR=${JMETER_DIR}
          SSH_USERNAME=${SSH_USERNAME}
          SSH_PASSWORD=${SSH_PASSWORD}
          TEST_PLAN_PATH=${TEST_PLAN_PATH}
          RESULT_PATH=${RESULT_PATH}
          NUMBER_OF_THREADS=${NUMBER_OF_THREADS}
          SSH_PORT_MASTER=${SSH_PORT_MASTER}
          SSH_PORT_SLAVE1=${SSH_PORT_SLAVE1}
          SSH_PORT_SLAVE2=${SSH_PORT_SLAVE2}
          EOF
          
      - name: Install npm dependencies
        working-directory: ./distributed-performance-testing
        run: npm install

      - name: Check Docker installation
        run: |
          docker --version
          docker compose version || docker-compose version
          
      - name: Build and start Docker containers
        working-directory: ./distributed-performance-testing
        run: docker compose up --build -d

      - name: Run node index.js
        working-directory: ./distributed-performance-testing/src
        run: node index.js  

      - name: Jmeter log
        uses: actions/upload-artifact@v4
        with:
          name: JMeter
          path: ./distributed-performance-testing/src/

      - name: Check if remote-metrics.log exists
        working-directory: ./distributed-performance-testing/src
        run: |
          ls
          if [ -f remote-metrics.log ]; then
            echo "File exists"
          else
            echo "File missing"; exit 1
          fi

      - name: Copy remote-metrics.log file to AiPerfTestAnalysis/inputs
        run: |
          cp ./distributed-performance-testing/src/remote-metrics.log ./AiPerfTestAnalysis/src/inputs/

      - name: Log & Statistics Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: distributed-test-results
          path: ./distributed-performance-testing/test-files/

      - name: Copy logs files to AiPerfTestAnalysis/inputs
        run: |
          cp ./distributed-performance-testing/test-files/*.log ./AiPerfTestAnalysis/src/inputs/

      - name: Copy statistics files to AiPerfTestAnalysis/inputs
        run: |
          find ./distributed-performance-testing/test-files/ -type f -path "*/html_report*/statistics.json" -exec cp {} ./AiPerfTestAnalysis/src/inputs/ \; || echo "No statistics.json found"

      - name: Create .env file inside AiPerfTestAnalysis
        working-directory: ./AiPerfTestAnalysis
        run: |
          ls
          cat <<EOF > .env
          GEMINI_MODEL_ID=${GEMINI_MODEL_ID}
          GEMINI_API_KEY=${GEMINI_API_KEY}
          PYTHONPATH=${PYTHONPATH}
          EOF

      - name: Extract Values from remote-metrics.log file
        working-directory: ./AiPerfTestAnalysis/src/inputs/
        run: |
          CPU = $(jq -r 'select(.message.label == "Available CPUs (cores)") | .message.output' "remote-metrics.log" | head -1)
          Memory = $(jq -r 'select(.message.label == "Total Memory (GB)") | .message.output' "remote-metrics.log" | head -1)
          DiskType = $(jq -r 'select(.message.label == "Disk Type") | .message.output' "remote-metrics.log" | head -1)

          echo "CPU=$CPU" >> ../.env
          echo "Memory=${MEMORY}GB" >> ../.env
          echo "DiskType=$DISK_TYPE" >> ../.env
          
          echo "âœ… Appended to ../.env file:"
          cat ../.env

      - name: Check python & pip Version
        working-directory: ./AiPerfTestAnalysis
        run: |
          python3 --version
          pip --version

      - name: Install requirement dependencies
        working-directory: ./AiPerfTestAnalysis
        run: pip install -r requirements.txt

      - name: Checking all files inside input folder
        working-directory: ./AiPerfTestAnalysis/src/inputs
        run: ls

      - name: Run AIperf python code 
        working-directory: ./AiPerfTestAnalysis
        run: python3 app.py

      - name: Recommedation Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: AI Perf Analysis recommendation file
          path: ./AiPerfTestAnalysis/src/outputs
